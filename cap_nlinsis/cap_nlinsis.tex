%Este trabalho está licenciado sob a Licença Creative Commons Atribuição-NãoComercial-CompartilhaIgual 4.0 Internacional. Para ver uma cópia desta licença, visite \url{http://creativecommons.org/licenses/by-nc-sa/4.0/} ou envie uma carta para Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.

\documentclass[main.tex]{subfiles}
\begin{document}
\chapter{Solução de sistemas de equações não lineares}

O método de Newton aplicado a encontrar a raiz $x^*$ da função $y=f(x)$ estudado na primeira área de nossa disciplina consiste em um processo iterativo. Em cada passo deste processo, dispomos de uma aproximação $x^{(k)}$ para $x^*$ e construímos uma aproximação $x^{(k+1)}$.  Cada passo do método de Newton envolve os seguintes procedimentos:
\begin{itemize}
\item Linearização da função $f(x)$ no ponto $x^{(k)}$: $f(x)= f(x^{(k)})+ (x-x^{(k)}) f'(x^{(k)}) + O\left(|x-x^{(k)}|^2\right) $
\item A aproximação $x^{(k+1)}$ é definida como o valor de $x$ em que a linearização $f(x^{(k)})+ (x-x^{(k)}) f'(x^{(k)})$ passa por zero.
\end{itemize}

{\bf Observação:} $y=f(x^{(k)})+ (x-x^{(k)}) f'(x^{(k)})$ é a equação da reta que tangencia a curva $y=f(x)$ no ponto $(x^{(k)},f(x^{(k)}))$.


Queremos, agora, generalizar o método de Newton a fim de resolver problemas de várias equações e várias incógnitas, ou seja, encontrar $x_1,x_2,\ldots x_n$ que satisfazem as seguinte equações:

\begin{eqnarray*}
f_1(x_1,x_2,\ldots,x_n)&=&0\\
f_2(x_1,x_2,\ldots,x_n)&=&0\\
&\vdots&\\
f_n(x_1,x_2,\ldots,x_n)&=&0
\end{eqnarray*}

Podemos escrever este problema na forma vetorial definindo o vetor $x=[x_1,x_2,\ldots,x_n]^T$ e a função vetorial
$$F(x)=\left[
\begin{array}{c}
f_1(x_1,x_2,\ldots,x_n)\\
f_2(x_1,x_2,\ldots,x_n)\\
\vdots\\
f_n(x_1,x_2,\ldots,x_n)
\end{array}
\right]$$

\begin{ex} Suponha que queiramos resolver numericamente os seguinte sistema de duas equações e duas incógnitas:
\begin{eqnarray*}
\frac{x_1^2}{3}+x_2^2&=&1\\
x_1^2+\frac{x_2^2}{4}&=&1
\end{eqnarray*}
Então definimos

$$F(x)=\left[
\begin{array}{c}
\frac{x_1^2}{3}+x_2^2-1\\~\\
x_1^2+\frac{x_2^2}{4}-1
\end{array}
\right]$$
\end{ex}
Neste momento, dispomos de um problema na forma $F(x)=0$ e precisamos desenvolver uma técnica para linearizar a função $F(x)$. Para tal, precisamos de alguns conceitos do Cálculo II.

Observe que $F(x)-F(x^{(0)})$ pode ser escrito como

$$F(x)-F(x^{(0)})=\left[
\begin{array}{c}
f_1(x_1,x_2,\ldots,x_n)-f_1(x_1^{(0)},x_2^{(0)},\ldots,x_n^{(0)})\\
f_2(x_1,x_2,\ldots,x_n)-f_2(x_1^{(0)},x_2^{(0)},\ldots,x_n^{(0)})\\
\vdots\\
f_n(x_1,x_2,\ldots,x_n)-f_n(x_1^{(0)},x_2^{(0)},\ldots,x_n^{(0)})
\end{array}
\right]$$

Usamos a regra da cadeia
$$df_i = \frac{\partial f_i}{\partial x_1} dx_1+\frac{\partial f_i}{\partial x_2} dx_2+\cdots + \frac{\partial f_i}{\partial x_n} dx_n=\sum_{j=1}^n\frac{\partial f_i}{\partial x_j} dx_j$$
e aproximamos as diferenças por derivadas parciais:
$$ f_i(x_1,x_2,\ldots,x_n)-f_i(x_1^{(0)},x_2^{(0)},\ldots,x_n^{(0)})\approx \sum_{j=1}^n \frac{\partial f_i}{\partial x_j}\left(x_j-x_j^{(0)}\right)$$
Portanto,
\begin{equation}\label{eq_approx_newton}F(x)-F(x^{(0)})\approx \left[
\begin{array}{ccccc}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\cdots&\frac{\partial f_1}{\partial x_n}\\~\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\cdots&\frac{\partial f_2}{\partial x_n}\\~\\
\vdots&\vdots&\ddots&\vdots\\~\\
\frac{\partial f_n}{\partial x_1}&\frac{\partial f_n}{\partial x_2}&\cdots&\frac{\partial f_n}{\partial x_n}\\~\\
\end{array}
\right]\left[
\begin{array}{c}
x_1-x_1^{(0)}\\~~\\
x_2-x_2^{(0)}\\~~\\
\vdots\\~~\\
x_n-x_n^{(0)}
\end{array}
\right]
\end{equation}

Definimos então a matriz jacobiana por
$$J_F= \frac{\partial(f_1,f_2,\ldots,f_n)}{\partial(x_1,x_2,\ldots,x_n)}=\left[
\begin{array}{ccccc}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\cdots&\frac{\partial f_1}{\partial x_n}\\~\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\cdots&\frac{\partial f_2}{\partial x_n}\\~\\
\vdots&\vdots&\ddots&\vdots\\~\\
\frac{\partial f_n}{\partial x_1}&\frac{\partial f_n}{\partial x_2}&\cdots&\frac{\partial f_n}{\partial x_n}\\~\\
\end{array}
\right]
$$
A matriz jacobiana de uma função ou simplesmente, o Jacobiano de uma função $F(x)$ é a matriz formada pelas suas derivadas parciais:
$$\left(J_F\right)_{ij}=\frac{\partial f_i}{\partial x_j}$$

Nestes termos podemos reescrever (\ref{eq_approx_newton}) como
$$F(x)\approx F(x^{(0)}) + J_F(x^{(0)}) (x-x^{(0)})$$
Esta expressão é chama de linearização de $F(x)$ no ponto $x^{(0)}$ e generaliza a linearização em uma dimensão dada por $f(x)\approx f(x^{(0)})+f'(x^{(0)}) (x-x^{(0)})$



\section{O método de  Newton para sistemas}
Vamos agora construir o método de Newton-Raphson, ou seja, o método de Newton generalizado para sistemas. Assumimos, portanto, que a função $F(x)$ é diferenciável e que existe um ponto $x^*$ tal que $F(x^*)=0$. Seja $x^{(k)}$ uma aproximação para $x^*$, queremos construir uma nova aproximação $x^{(k+1)}$ através da linearização de $F(x)$ no ponto $x^{(k)}$.

\begin{itemize}
\item Linearização da função $F(x)$ no ponto $x^{(k)}$: $F(x)= F(x^{(k)})+ J_F\left(x^{(k)}\right) \left(x-x^{(k)}\right)  + O\left(\|x-x^{(k)}\|^2\right) $
\item A aproximação $x^{(k)}$ é definida como o ponto $x$ em que a linearização $F(x^{(k)})+ J_F\left(x^{(k)}\right) \left(x-x^{(k)}\right)$ é nula, ou seja:
$$F(x^{(k)})+ J_F\left(x^{(k)}\right) \left(x^{(k+1)}-x^{(k)}\right)=0$$
\end{itemize}

Supondo que a matriz jacobina seja inversível no ponto $x^{(k)}$, temos:
\begin{eqnarray*}
J_F\left(x^{(k)}\right) \left(x^{(k+1)}-x^{(k)}\right)&=&-F(x^{(k)})\\
x^{(k+1)}-x^{(k)}&=&-J_F^{-1}\left(x^{(k)}\right)F(x^{(k)})\\
x^{(k+1)}&=&x^{(k)}-J_F^{-1}\left(x^{(k)}\right)F(x^{(k)})
\end{eqnarray*}

Desta forma, o método iterativo de Newton-Raphson para encontrar as raízes de $F(x)=0$ é dado por
$$
\left\{
\begin{array}{rcl}
x^{(k+1)}&=&x^{(k)}-J_F^{-1}\left(x^{(k)}\right)F(x^{(k)}),~~ n\geq 0\\
~\\
x^{(0)}&=&\hbox{dado inicial}
\end{array}
\right.$$

\begin{obs} Usamos subíndices para indicar o elemento de um vetor e super-índices para indicar o passo da iteração. Assim $x^{(k)}$ se refere à iteração $k$ e $x_i^{(k)}$ se refere à componente $i$ no vetor $x^{(k)}$.
\end{obs}
\begin{obs} A notação $J_F^{-1}\left(x^{(k)}\right)$ enfatiza que a jacobiana deve ser calculada a cada passo.
\end{obs}
\begin{obs} Podemos definir o passo $\Delta^{(k)}$ como
$$\Delta^{(k)}= x^{(k+1)}-x^{(k)}$$
Assim, $\Delta^{(k)}=-J_F^{-1}\left(x^{(k)}\right)F(x^{(k)})$, ou seja, $\Delta^{(k)}$ resolve o problema linear:
$$J_F\left(x^{(k)}\right)\Delta^{(k)}= - F(x^{(k)})$$
Em geral, é menos custoso resolver o sistema acima do que calcular o inverso da jacobiana e multiplicar pelo vetor $F(x^{(k)})$.
\end{obs}

\begin{ex} Retornamos ao nosso exemplo inicial, isto é, resolver numericamente os seguinte sistema não-linear:
\begin{eqnarray*}
\frac{x_1^2}{3}+x_2^2&=&1\\
x_1^2+\frac{x_2^2}{4}&=&1
\end{eqnarray*}
Para tal, definimos a função $F(x)$:
$$F(x)=\left[
\begin{array}{c}
\frac{x_1^2}{3}+x_2^2-1\\~\\
x_1^2+\frac{x_2^2}{4}-1
\end{array}
\right]$$
cuja jacobiana é
$$J_F=\left[
\begin{array}{cc}
\frac{2x_1}{3} & 2x_2\\~\\
2x_1&\frac{x_2}{2}
\end{array}
\right]
$$

Faremos a implementação numérica no Scilab. Para tal definimos as funções que implementarão $F(x)$ e a $J_F(x)$
\begin{verbatim}
function y=F(x)
    y(1)=x(1)^2/3+x(2)^2-1
    y(2)=x(1)^2+x(2)^2/4-1
endfunction

function y=JF(x)
    y(1,1)=2*x(1)/3
    y(1,2)=2*x(2)
    y(2,1)=2*x(1)
    y(2,2)=x(2)/2
endfunction
\end{verbatim}
Alternativamente, estas funções poderiam ser escritas como
\begin{verbatim}
function y=F(x)
    y=[x(1)^2/3+x(2)^2-1; x(1)^2+x(2)^2/4-1]
endfunction

function y=JF(x)
    y=[2*x(1)/3  2*x(2); 2*x(1) x(2)/2]
endfunction
\end{verbatim}
Desta forma, se $x$ é uma aproximação para a raiz, pode-se calcular a próxima aproximação através dos comandos:
\begin{verbatim}
delta=-JF(x)\F(x)
x=x+delta
\end{verbatim}
Ou simplesmente
\begin{verbatim}
x=x-JF(x)\F(x)
\end{verbatim}
Observe que as soluções exatas desse sistema são $\left(\pm \sqrt{\frac{9}{11}},\pm \sqrt{\frac{8}{11}}\right)$
\end{ex}


\begin{prob} Encontre uma aproximação para a solução do sistema
\begin{eqnarray*}
x_1^2=\cos(x_1x_2)+1\\
\sin(x_2)=2\cos(x_1)
\end{eqnarray*}
que fica próxima ao ponto $x_1=1.5$ e $x_2=.5$.
{\bf Resp:} (1,3468109,~0,4603195)
\end{prob}


{\bf Solução:}
Definimos a função $F(x)$ como
$$F(x)=\left[
\begin{array}{c}
x_1^2-\cos(x_1x_2)-1\\~\\
\sin(x_2)-2\cos(x_1)
\end{array}
\right]$$
cuja jacobiana é
$$J_F(x)=\left[
\begin{array}{cc}
2x_1 +x_2\sin(x_1x_2)&x_1\sin(x_1x_2)\\~\\
2\sin(x_1) & \cos(x_2)
\end{array}
\right]
$$

Implementamos no Scilab como
\begin{verbatim}
function y=F(x)
    y=[x(1)^2-cos(x(1)*x(2))-1 ; sin(x(2))-2*cos(x(1))]
endfunction

function y=JF(x)
    y=[2*x(1)+x(2)*sin(x(1)*x(2)) x(1)*sin(x(1)*x(2));2*sin(x(1)) cos(x(2))]
endfunction
\end{verbatim}

E agora, basta iterar:
\begin{verbatim}
x=[1.5; .5]
x=x-JF(x)\F(x) (5 vezes)
\end{verbatim}

\begin{prob} Encontre uma aproximação numérica para o seguinte problema não-linear de três equações e três incógnitas:
\begin{eqnarray*}
2x_1-x_2&=&\cos(x_1)\\
-x_1+2x_2-x_3&=&\cos(x_2)\\
-x_2+	x_3&=&\cos(x_3)
\end{eqnarray*}
Partindo das seguintes aproximações iniciais:
\begin{itemize}
\item[a)] $x^{(0)}=[1,~1,~1]^T$
\item[b)] $x^{(0)}=[-0,5,~-2,~-3]^T$
\item[c)] $x^{(0)}=[-2,~-3,~-4]^T$
\item[d)] $x^{(0)}=[0,~0,~0]^T$
\end{itemize}
\end{prob}
{\bf Implementação no scilab:}
\begin{verbatim}
function y=F(x)
y=[
   2*x(1)-x(2)-cos(x(1)) ;
   -x(1)+2*x(2)-x(3)-cos(x(2));
   -x(2)+x(3)-cos(x(3))]
endfunction

function y=JF(x)
y=[
   2+sin(x(1)) -1 0;
   -1 2+sin(x(2)) -1;
   0 -1 1+sin(x(3))]

endfunction
\end{verbatim}

\subsection{Algoritmo de Newton para Sistemas}

\verbatiminput{./cap_nlinsis/codes/metodo_de_newton/newton.sci}

\section{Linearização de uma função de várias variáveis, o gradiente e a Jacobiana*}
\subsection{O gradiente}

Considere primeiramente uma função $f:\mathbb{R}^n\to \mathbb{R}$, ou seja, uma função que mapeia n variáveis reais em um único real, por exemplo:
$$f(x)=x_1^2+x_2^2/4$$

Para construirmos a linearização, fixemos uma direção no espaço $\mathbb{R}^n$, ou seja um vetor $v$:
$$v=[v_1,  v_2,  \cdots,  v_n]^T$$

Queremos estudar como a função $f(x)$ varia quando ``andamos'' na direção $v$ a partir do ponto $x^{(0)}$. Para tal, inserimos um parâmetro  real pequeno $h$, dizemos que $$x=x^{(0)}+hv$$ e definimos a função auxiliar
$$g(h)=f(x^{0}+hv).$$
Observamos que a função $g(h)$ é uma função de $\mathbb{R}$ em $\mathbb{R}$.



A linearização de $g(h)$ em torno de $h=0$ é dada por

$$g(h)=g(0) + hg'(0) +O(h^2)$$
Observamos que $g(h)=f(x^{(0)}+hv)$ e $g(0)=f(x^{(0)})$. Precisamos calcular $g'(0)$:

\begin{eqnarray*}
g'(h)=\frac{d}{dh}g(h)=\frac{d}{dh}f(x^{(0)}+hv)
\end{eqnarray*}
Pela regra da cadeia temos:
\begin{eqnarray*}
\frac{d}{dh}f(x^{(0)}+hv)= \sum_{j=1}^n \frac{\partial f}{\partial x_j}\frac{d x_j}{d h}
\end{eqnarray*}

Observamos que $x_j=x^{(0)}_j+hv_j$, portanto
$$\frac{d x_j}{d h}=v_j$$
Assim:
\begin{eqnarray*}
\frac{d}{dh}f(x^{(0)}+hv)= \sum_{j=1}^n \frac{\partial f}{\partial x_j}v_j
\end{eqnarray*}
Observamos que esta expressão pode ser vista como o produto interno entre o gradiente de $f$ e o vetor v:
\begin{eqnarray*}
\nabla f = \left[
\begin{array}{c}
\frac{\partial f}{\partial x_1} \\~\\
\frac{\partial f}{\partial x_2} \\~\\
\vdots\\~\\
\frac{\partial f}{\partial x_n}
\end{array}
\right]~~~v=\left[
\begin{array}{c}
v_1\\~~\\
v_2\\~~\\
\vdots\\~\\
v_n
\end{array}
\right]
\end{eqnarray*}

Na notação cálculo vetorial escrevemos este produto interno como $\nabla f \cdot v = v \cdot \nabla f$ na notação de produto matricial, escrevemos $\left(\nabla f\right)^T v = v^T\nabla f$. Esta quantidade é conhecida como {\bf derivada direcional} de $f$ no ponto $x^{(0)}$ na direção $v$, sobretudo quando $\|v\|=1$.


Podemos escrever a linearização
$g(h)=g(0) + hg'(0) +O(h^2)$ como
$$f(x^{(0)}+hv)=f(x^{(0)})+ h \nabla^T\! f(x^{(0)})\!~v  + O(h^2)$$

Finalmente, escrevemos $x=x^{(0)}+hv$, ou seja, $hv=x-x^{(0)}$
$$f(x)=f(x^{(0)})+ \nabla^T\! f(x^{(0)})\!~(x-x^{(0)})   + O(\|x-x^{(0)}\|^2)$$

\begin{obs} Observe a semelhança com a linearização no caso em uma dimensão. A notação $\nabla^T\! f(x^{(0)})$ é o transposto do vetor gradiente associado à função $f(x)$ no ponto $x^{(0)}$:
$$\nabla^T f(x^{(0)})=\left[\frac{\partial f\left(x^{(0)}\right)}{\partial x_1},~~ \frac{\partial f\left(x^{(0)}\right)}{\partial x_2},~~ \cdots,~\frac{\partial f\left(x^{(0)}\right)}{\partial x_n}\right]$$
\end{obs}

\subsection{A matriz jacobiana}
Interessamo-nos, agora, pela linearização da função $F:\mathbb{R}^n\to \mathbb{R}^n$. Lembramos que $F(x)$ pode ser escrita como um vetor de funções $f_j:\mathbb{R^n}\to\mathbb{R}$:
$$F(x)=\left[
\begin{array}{c}
f_1(x)\\~\\
f_2(x)\\~\\
\vdots\\~\\
f_n(x)
\end{array}
\right]$$
Linearizando cada uma das funções $f_j$, temos:
\begin{eqnarray*}
F(x)&=&\underbrace{\left[
\begin{array}{c}
f_1\left(x^{(0)}\right)+\nabla^T\! f_1(x^{(0)})\!~\left(x-x^{(0)}\right)   + O(\|x-x^{(0)}\|^2)\\~\\
f_2\left(x^{(0)}\right)+\nabla^T\! f_2(x^{(0)})\!~\left(x-x^{(0)}\right)   + O(\|x-x^{(0)}\|^2)\\~\\
\vdots\\~\\
f_n\left(x^{(0)}\right)+\nabla^T\! f_n(x^{(0)})\!~\left(x-x^{(0)}\right)   + O(\|x-x^{(0)}\|^2)
\end{array}
\right]}_{\hbox{Vetor coluna}}\\
&~&\\
&~&\\
&~&\\
&=&\underbrace{\left[
\begin{array}{c}
f_1\left(x^{(0)}\right)\\~\\
f_2\left(x^{(0)}\right)\\~\\
\vdots\\~\\
f_n\left(x^{(0)}\right)
\end{array}
\right]}_{\hbox{Vetor coluna}}+
\underbrace{\left[
\begin{array}{c}\nabla^T\! f_1(x^{(0)})\\~~\\
\nabla^T\! f_2(x^{(0)})\\~~\\
\vdots\\~~\\
\nabla^T\! f_n(x^{(0)})
\end{array}
\right]}_{\hbox{Matriz jacobiana}}\underbrace{\left(x-x^{(0)}\right)}_{\hbox{Vetor coluna}}+O(\|x-x^{(0)}\|^2)
\end{eqnarray*}

Podemos escrever a linearização de $F(x)$ na seguinte forma mais enxuta:
$$F(x)=F\left(x^{(0)}\right)+ J_F(x^{(0)})\left(x-x^{(0)}\right) + O\left(\left\|x-x^{(0)}\right\|^2\right) $$

A matriz jacobiana $J_F$ é matriz cujas linhas são os gradientes transpostos de $f_j$, ou seja:
$$J_F= \frac{\partial(f_1,f_2,\ldots,f_n)}{\partial(x_1,x_2,\ldots,x_n)}=\left[
\begin{array}{ccccc}
\frac{\partial f_1}{\partial x_1}&\frac{\partial f_1}{\partial x_2}&\cdots&\frac{\partial f_1}{\partial x_n}\\~\\
\frac{\partial f_2}{\partial x_1}&\frac{\partial f_2}{\partial x_2}&\cdots&\frac{\partial f_2}{\partial x_n}\\~\\
\vdots&\vdots&\ddots&\vdots\\~\\
\frac{\partial f_n}{\partial x_1}&\frac{\partial f_n}{\partial x_2}&\cdots&\frac{\partial f_n}{\partial x_n}\\~\\
\end{array}
\right]
$$
A matriz jacobiana de uma função ou simplesmente, o Jacobiano de uma função $F(x)$ é a matriz formada pelas suas derivadas parciais:
$$\left(J_F\right)_{ij}=\frac{\partial f_i}{\partial x_j}$$


\begin{ex} Calcule a matriz jacobiana da função
$$F(x)=\left[
\begin{array}{c}
\frac{x_1^2}{3}+x_2^2-1\\~\\
x_1^2+\frac{x_2^2}{4}-1
\end{array}
\right]$$

$$J_F=\left[
\begin{array}{cc}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2}\\~\\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2}\\
\end{array}
\right]=\left[
\begin{array}{cc}
\frac{2x_1}{3} & 2x_2\\~\\
2x_1&\frac{x_2}{2}
\end{array}
\right]
$$
\end{ex}
\end{document} 